{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required packages and functions.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This data obtained from open source at: https://www.kaggle.com/datasets/zaraavagyan/weathercsv?resource=download\n",
    "## I do not claim ownership of the data, all rights belong to the respective owner(s).\n",
    "\n",
    "## Downloading the data.\n",
    "subprocess.run(['kaggle', 'datasets', 'download', '-d', 'zaraavagyan/weathercsv'], check=True)\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(\"weathercsv.zip\", 'r') as ZIPPED:\n",
    "    ZIPPED.extractall(\"./Assets/\")\n",
    "\n",
    "os.system(f\"del weathercsv.zip\")\n",
    "\n",
    "## Loading data\n",
    "weatherData = pd.read_csv(\"./Assets/weather.csv\") ## importing data\n",
    "\n",
    "\n",
    "## Dropping NA data rows.\n",
    "weatherData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Firstly visualizing the data, to see what the likely key component interactions are.\n",
    "# sns.heatmap(weatherData.corr(), annot=False)\n",
    "weatherData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying the categorical features.\n",
    "\n",
    "categoricalFeatures = [column_name for column_name in weatherData.columns if weatherData[column_name].dtype == 'O']\n",
    "print(\"Amount of Categorical Features: {}\".format(len(categoricalFeatures)))\n",
    "print(\"Categorical Features: \",categoricalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cardinality check.\n",
    "for each_feature in categoricalFeatures:\n",
    "   unique_values = len(weatherData[each_feature].unique())\n",
    "   print(\"Cardinality(no. of unique values) of {} are: {}\".format(each_feature, unique_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping high cardinality columns.\n",
    "weatherData.drop(['WindGustDir','WindDir9am','WindDir3pm'], axis = 1, inplace = True)\n",
    "weatherData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for any NULL values in data, should be 0 as all NA/NULL values were dropped.\n",
    "categoricalFeatures = [column_name for column_name in weatherData.columns if weatherData[column_name].dtype == 'O']\n",
    "weatherData[categoricalFeatures].isnull().sum()\n",
    "\n",
    "numericalFeatures = [column_name for column_name in weatherData.columns if weatherData[column_name].dtype != 'O']\n",
    "weatherData[numericalFeatures].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying the numerical features.\n",
    "\n",
    "numericalFeatures = [column_name for column_name in weatherData.columns if weatherData[column_name].dtype != 'O']\n",
    "print(\"Amount of Numerical Features: {}\".format(len(numericalFeatures)))\n",
    "print(\"Numerical Features: \",numericalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature encoding.\n",
    "\n",
    "weatherData['RainToday'].replace({'No':0, 'Yes': 1}, inplace = True)\n",
    "\n",
    "weatherData['RainTomorrow'].replace({'No':0, 'Yes': 1}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assessing the correlation between the various parameters.\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(weatherData.corr(), linewidths=0.5, annot=False, fmt=\".2f\", cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantifying feature importance.\n",
    "\n",
    "X = weatherData.drop(['RainTomorrow'],axis=1)\n",
    "y = weatherData['RainTomorrow']\n",
    "etr_model = ExtraTreesRegressor()\n",
    "etr_model.fit(X,y)\n",
    "etr_model.feature_importances_\n",
    "\n",
    "## Visualizing the feature importance.\n",
    "feature_imp = pd.Series(etr_model.feature_importances_,index=X.columns)\n",
    "feature_imp.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making training models.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
    "print(\"Length of Training Data: {}\".format(len(X_train)))\n",
    "print(\"Length of Testing Data: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making KN model to include: PCA analysis, feature scaling, and the KN model itself.\n",
    "KNC_Pipeline = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        PCA(),\n",
    "        KNeighborsClassifier(n_neighbors=10)\n",
    "    ),\n",
    "    {\n",
    "        \"pca__n_components\" : range(1, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "## Fitting the model and viewing the score.\n",
    "KNC_Pipeline.fit(X_train, y_train)\n",
    "print(f\"Score for K-Neighbors Classifier is: {KNC_Pipeline.score(X_test, y_test):.3}\")\n",
    "print(f\"From PCA analysis, the best estimator is: {KNC_Pipeline.best_estimator_['pca'].n_components_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the KN model by predicting values.\n",
    "y_PredictionKNC = KNC_Pipeline.predict(X_test)\n",
    "\n",
    "## Assessing the KN model score.\n",
    "print(\"Accuracy Score: {:.3}\".format(accuracy_score(y_test,y_PredictionKNC)))\n",
    "\n",
    "## Visualizing with a confusion matrix.\n",
    "cm = confusion_matrix(y_test, y_PredictionKNC)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the KN model for over/under fitting.\n",
    "print(\"Train Data Score: {}\".format(KNC_Pipeline.score(X_train, y_train)))\n",
    "print(\"Test Data Score: {}\".format(KNC_Pipeline.score(X_test, y_test)))\n",
    "\n",
    "## Checking if score can be improved for KN classifier.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(KNC_Pipeline, X_train, y_train, cv = 5, scoring='accuracy')\n",
    "print('Cross-validation scores:{}'.format(scores))\n",
    "print('Average cross-validation score: {}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving out the trained KN classifier.\n",
    "\n",
    "with open('./Classifiers/KNC_Rain.pkl', 'wb') as file:\n",
    "    pickle.dump(KNC_Pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking to see how many features should be kept.\n",
    "SVC_CheckPipeline = Pipeline(\n",
    "    [   \n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"anova\", SelectPercentile(chi2)),\n",
    "        (\"svc\", SVC(gamma=\"auto\",random_state = 42,decision_function_shape='ovr',kernel=\"linear\"),),\n",
    "    ]\n",
    ")\n",
    "scoremeans = list()\n",
    "scoreSTDs = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "## Looping over each percentile and checking the accuracy.\n",
    "for percentile in percentiles:\n",
    "    SVC_CheckPipeline.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(SVC_CheckPipeline, X, y)\n",
    "    scoremeans.append(this_scores.mean())\n",
    "    scoreSTDs.append(this_scores.std())\n",
    "\n",
    "## Outputting the accuracies as a graph.\n",
    "plt.errorbar(percentiles, scoremeans, np.array(scoreSTDs))\n",
    "plt.title(\"How the performance of the SVC-Anova changes, \\nbased on varying the percentile of features selected\")\n",
    "plt.xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking 40% of features, as this was the highest accuracy based on the graph made in the previous step.\n",
    "## Also re-making weatherData to incorporate this change.\n",
    "\n",
    "featureSelection = VarianceThreshold(0.4)\n",
    "featureSelection.fit_transform(weatherData)\n",
    "\n",
    "X = weatherData.drop(['RainTomorrow'],axis=1)\n",
    "y = weatherData['RainTomorrow']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.6, random_state = 0)\n",
    "\n",
    "SVC_Pipeline = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        PCA(),\n",
    "        SVC(gamma=10,random_state = 42,decision_function_shape='ovr',kernel=\"linear\",tol=0.1,C=0.5),\n",
    "    ),\n",
    "    {\n",
    "        \"pca__n_components\" : range(1, 8),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "SVC_Pipeline.fit(X_train, y_train)\n",
    "print(f\"Score for SVC Classifier is: {SVC_Pipeline.score(X_test, y_test):.3}\")\n",
    "print(f\"From PCA analysis, the best estimator is: {SVC_Pipeline.best_estimator_['pca'].n_components_}\")\n",
    "\n",
    "## Checking correct and incorrect predictions with a confusion matrix.\n",
    "y_Prediction = SVC_Pipeline.predict(X_test)\n",
    "confusionMatrix_SVC = confusion_matrix(y_test, y_Prediction)\n",
    "cm_display = ConfusionMatrixDisplay(confusionMatrix_SVC).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for over/under fitting.\n",
    "print(\"Train Data Score: {}\".format(SVC_Pipeline.score(X_train, y_train)))\n",
    "print(\"Test Data Score: {}\".format(SVC_Pipeline.score(X_test, y_test)))\n",
    "\n",
    "## Checking if score can be improved for KN classifier.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(SVC_Pipeline, X_train, y_train, cv = 5, scoring='accuracy')\n",
    "print('Cross-validation scores:{}'.format(scores))\n",
    "print('Average cross-validation score: {}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving out the trained classifier.\n",
    "\n",
    "with open('./classifiers/SVC_Rain.pkl', 'wb') as file:\n",
    "    pickle.dump(SVC_Pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusions\n",
    "\n",
    "SVC was found to be the better model, with a score of 0.944 vs KN with a score of 0.894.\n",
    "If these scores appear different to the ones you find, there is a chance that the downloaded data set has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a fake data set\n",
    "## Define column names\n",
    "import random\n",
    "\n",
    "rangeToUse = 20\n",
    "\n",
    "columns = [\n",
    "    'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', \n",
    "    'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n",
    "    'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', \n",
    "    'Temp9am', 'Temp3pm', 'RainToday', 'RISK_MM'\n",
    "]\n",
    "\n",
    "## Create an empty DataFrame with specified columns\n",
    "fakeDf = pd.DataFrame(columns=columns)\n",
    "\n",
    "## Defining range for each column\n",
    "ranges = {\n",
    "    'MinTemp': (-10, 40), 'MaxTemp': (-10, 40), 'Rainfall': (0, 10), \n",
    "    'Evaporation': (0, 12), 'Sunshine': (0, 15), 'WindGustSpeed': (0, 100), \n",
    "    'WindSpeed9am': (0, 50), 'WindSpeed3pm': (0, 50), 'Humidity9am': (70, 100), \n",
    "    'Humidity3pm': (70, 100), 'Pressure9am': (980, 1100), 'Pressure3pm': (980, 1100), \n",
    "    'Cloud9am': (0, 9), 'Cloud3pm': (0, 9), 'Temp9am': (-10, 40), 'Temp3pm': (-10, 40), \n",
    "    'RainToday': (0, 1), 'RISK_MM': (0, 100)\n",
    "}\n",
    "\n",
    "## Generate random values for each column\n",
    "for col in columns:\n",
    "    if col != \"RainToday\":\n",
    "        minVal, maxVal = ranges[col]\n",
    "        fakeDf[col] = [round(random.uniform(minVal, maxVal), 1) for _ in range(rangeToUse)]\n",
    "    else:\n",
    "        fakeDf[col] = [random.randint(0,1) for _ in range(rangeToUse)]\n",
    "\n",
    "fakeDf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Using the SVC model:\n",
    "PredictedForecastSVC = SVC_Pipeline.best_estimator_.predict(fakeDf)\n",
    "\n",
    "\n",
    "## Using the KN model:\n",
    "PredictedForecastKN = KNC_Pipeline.best_estimator_.predict(fakeDf)\n",
    "\n",
    "\n",
    "fakeDf['PredictedForecastSVC'] = PredictedForecastSVC\n",
    "fakeDf['PredictedForecastKN'] = PredictedForecastKN\n",
    "\n",
    "fakeDf['PredictionMatch'] = False\n",
    "\n",
    "\n",
    "for idx, row in fakeDf.iterrows():\n",
    "    if row['PredictedForecastKN'] == row['PredictedForecastSVC']:\n",
    "        fakeDf.at[idx,'PredictionMatch'] = True\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "fakeDf.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction match from each ML model\n",
    "falseCount = len(fakeDf[fakeDf['PredictionMatch'] == False])\n",
    "trueCount = len(fakeDf[fakeDf['PredictionMatch'] == True])\n",
    "percentageMatch =(trueCount/rangeToUse) * 100\n",
    "print(f\"Match percent: {percentageMatch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1870f1194fb5b3d43b6d32e845741389586dbe9c4e1e45e17e0f6602cfe22778"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
